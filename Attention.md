# Introduction
<br/>Recurrent Neural Networks (RNNs) are great for tasks like language modeling but have limits due to sequential processing.
<br/>It introduces the Transformer model, which uses attention mechanisms instead of recurrence, making it faster and better at translation tasks.

-----------------

# Background
<br/> There are alternative approaches to reduce sequential computation in neural network architectures, such as the Extended Neural GPU, ByteNet, and ConvS2S, which utilize convolutional neural networks for parallel computation. 
<br/> These models struggle with learning dependencies between distant positions.
<br/>
<br/>
<br/> The Transformer model uses self-attention.
<br/> It achieves efficient computation of representations.
<br/> This approach eliminates the need for sequential recurrence or convolution.
<br/> It represents a significant advancement in transduction models.
<br/>
